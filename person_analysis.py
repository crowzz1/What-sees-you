"""
Person and Face Analysis System
- Body: Pose, Clothing, Color, Body Type, Keypoints
- Face: Age, Emotion, Features
Using YOLOv8-Pose + InsightFace + FER + DeepFace

This module contains the core analyzer class used by the multi-camera system.
"""

import cv2
import numpy as np
import torch
from ultralytics import YOLO
from sklearn.cluster import KMeans
import time
import os
import random

# TensorFlow GPU Memory Growth (Prevent DeepFace from hogging all VRAM)
try:
    import tensorflow as tf
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
except ImportError:
    pass
except Exception as e:
    print(f"Warning: Could not set TF memory growth: {e}")

# Depth estimation (optional)
try:
    import torch.nn.functional as F
    DEPTH_AVAILABLE = True
except ImportError:
    DEPTH_AVAILABLE = False

# TouchDesigner transmitter (Moved to integrations/)
TD_AVAILABLE = False

# Oscilloscope & AI Description removed
OSCILLOSCOPE_AVAILABLE = False
ENABLE_OSCILLOSCOPE_AUDIO = False
AI_DESC_AVAILABLE = False

# Suppress warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Suppress libpng warnings (iCCP profile warnings)
# These warnings come from OpenCV/PIL when loading PNG images in model files
# They don't affect functionality, just suppress the output
import warnings
warnings.filterwarnings('ignore', category=UserWarning, message='.*iCCP.*')

# Also suppress stderr output from libpng (if needed)
# This redirects stderr temporarily during model loading
import sys
from contextlib import contextmanager

@contextmanager
def suppress_stderr():
    """Temporarily suppress stderr output"""
    with open(os.devnull, 'w') as devnull:
        old_stderr = sys.stderr
        sys.stderr = devnull
        try:
            yield
        finally:
            sys.stderr = old_stderr

# InsightFace imports
try:
    import insightface
    from insightface.app import FaceAnalysis
    INSIGHTFACE_AVAILABLE = True
except ImportError:
    print("Warning: InsightFace not installed.")
    INSIGHTFACE_AVAILABLE = False

# HSEmotion imports (Primary for Emotion)
try:
    # Monkey Patch torch.load to disable weights_only check for HSEmotion
    _original_load = torch.load
    def _safe_load(*args, **kwargs):
        if 'weights_only' not in kwargs:
            kwargs['weights_only'] = False
        return _original_load(*args, **kwargs)
    torch.load = _safe_load
    
    from hsemotion.facial_emotions import HSEmotionRecognizer
    HSEMOTION_AVAILABLE = True
except ImportError:
    print("Warning: hsemotion not installed.")
    HSEMOTION_AVAILABLE = False

# Clean up DeepFace/FER references
DEEPFACE_AVAILABLE = False
FER_AVAILABLE = False

class CompletePersonFaceAnalyzer:
    """Complete person and face analysis with all attributes"""
    
    def __init__(self, show_keypoints=True, show_skeleton=True):
        print("=" * 60)
        print("Complete Person + Face Analysis System")
        print("ALL ATTRIBUTES: Age, Emotion")
        print("=" * 60)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Device: {self.device}")
        
        if self.device.type == 'cuda':
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
        
        self.show_keypoints = show_keypoints
        self.show_skeleton = show_skeleton
        
        # Load YOLOv8-Pose
        print("Loading YOLOv8-Pose for body detection...")
        # Suppress libpng warnings during model loading
        with suppress_stderr():
            pose_path = 'models/yolov8n-pose.pt'
            if not os.path.exists(pose_path): pose_path = 'yolov8n-pose.pt'
            self.yolo_model = YOLO(pose_path)
        if self.device.type == 'cuda':
            self.yolo_model.to(self.device)
        print("  âœ“ YOLOv8-Pose loaded!")
        
        # Load YOLOv8-Seg for accurate person segmentation (for visual effects)
        print("Loading YOLOv8-Seg for person segmentation...")
        try:
            with suppress_stderr():
                seg_path = 'models/yolov8n-seg.pt'
                if not os.path.exists(seg_path): seg_path = 'yolov8n-seg.pt'
                self.yolo_seg_model = YOLO(seg_path)
            if self.device.type == 'cuda':
                self.yolo_seg_model.to(self.device)
            print("  âœ“ YOLOv8-Seg loaded!")
            self.segmentation_enabled = True
        except Exception as e:
            print(f"  âš  YOLOv8-Seg failed to load: {e}")
            print("  â†’ Visual effects will use keypoint-based silhouette")
            self.segmentation_enabled = False
        
        # Load MiDaS for depth estimation (for depth-based visual effects)
        # æ·±åº¦æ¨¡å¼å·²æ³¨é‡Š
        # print("Loading MiDaS for depth estimation...")
        # try:
        #     # Check if timm is available (required for MiDaS)
        #     try:
        #         import timm
        #     except ImportError:
        #         print("  âš  timm library not found. Install with: pip install timm>=0.9.0")
        #         print("  â†’ Depth effects will be disabled")
        #         self.depth_enabled = False
        #         self.midas = None
        #         self.midas_transform = None
        #         self.depth_cache = None
        #         self.depth_cache_counter = 0
        #         return
        #     
        #     # Use MiDaS small model for real-time performance
        #     print("  â†’ Downloading MiDaS model from PyTorch Hub (first time may take a while)...")
        #     self.midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', trust_repo=True)
        #     self.midas.to(self.device)
        #     self.midas.eval()
        #     
        #     # Load MiDaS transforms
        #     midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms', trust_repo=True)
        #     self.midas_transform = midas_transforms.small_transform
        #     
        #     # Initialize depth cache
        #     self.depth_cache = None
        #     self.depth_cache_counter = 0
        #     
        #     print("  âœ“ MiDaS depth model loaded!")
        #     self.depth_enabled = True
        # except Exception as e:
        #     print(f"  âš  MiDaS failed to load: {e}")
        #     print("  â†’ Depth effects will be disabled")
        #     print("  â†’ Troubleshooting:")
        #     print("    1. Check internet connection (model downloads from PyTorch Hub)")
        #     print("    2. Install timm: pip install timm>=0.9.0")
        #     print("    3. If network issue, try: pip install --upgrade torch torchvision")
        #     self.depth_enabled = False
        #     self.midas = None
        #     self.midas_transform = None
        #     self.depth_cache = None
        #     self.depth_cache_counter = 0
        
        # æ·±åº¦æ¨¡å¼å·²ç¦ç”¨
        self.depth_enabled = False
        self.midas = None
        self.midas_transform = None
        self.depth_cache = None
        self.depth_cache_counter = 0
        
        # Load InsightFace
        if INSIGHTFACE_AVAILABLE:
            print("Loading InsightFace for face analysis...")
            try:
                self.face_app = FaceAnalysis(
                    name='buffalo_l',
                    providers=['CUDAExecutionProvider', 'CPUExecutionProvider'] if self.device.type == 'cuda' else ['CPUExecutionProvider']
                )
                # Use 640x640 for stability (official recommendation)
                # Square input avoids broadcast shape errors with 1920x1080 frames
                self.face_app.prepare(ctx_id=0 if self.device.type == 'cuda' else -1, det_size=(640, 640))
                print("  âœ“ InsightFace loaded! (det_size: 640x640)")
                self.face_enabled = True
            except Exception as e:
                print(f"  âœ— InsightFace failed: {e}")
                self.face_enabled = False
        else:
            self.face_enabled = False
        
        # Load HSEmotion for emotion recognition
        if HSEMOTION_AVAILABLE:
            print("Loading HSEmotion for emotion recognition...")
            try:
                # ä½¿ç”¨æ¨èçš„æ¨¡å‹ (æ”¯æŒ GPU)
                self.emotion_detector = HSEmotionRecognizer(
                    model_name='enet_b0_8_best_vgaf', 
                    device='cuda' if self.device.type == 'cuda' else 'cpu'
                )
                print("  âœ“ HSEmotion loaded! (EfficientNet-B0)")
                self.emotion_enabled = True
            except Exception as e:
                print(f"  âœ— HSEmotion failed: {e}")
                self.emotion_enabled = False
        else:
            self.emotion_enabled = False
        
        # DeepFace is only used for emotion detection now
        # No race detection needed
        
        # Keypoint names (COCO format - 17 points)
        self.keypoint_names = [
            'Nose', 'Left Eye', 'Right Eye', 'Left Ear', 'Right Ear',
            'Left Shoulder', 'Right Shoulder', 'Left Elbow', 'Right Elbow',
            'Left Wrist', 'Right Wrist', 'Left Hip', 'Right Hip',
            'Left Knee', 'Right Knee', 'Left Ankle', 'Right Ankle'
        ]
        
        # Skeleton connections
        self.skeleton = [
            [15, 13], [13, 11], [16, 14], [14, 12], [11, 12],
            [5, 11], [6, 12], [5, 6],
            [5, 7], [6, 8], [7, 9], [8, 10],
            [0, 1], [0, 2], [1, 3], [2, 4], [3, 5], [4, 6]
        ]
        
        # Colors for keypoints
        self.keypoint_colors = [
            (255, 0, 0), (255, 128, 0), (255, 255, 0), (128, 255, 0),
            (0, 255, 0), (0, 255, 128), (0, 255, 255), (0, 128, 255),
            (0, 0, 255), (128, 0, 255), (255, 0, 255), (255, 0, 128),
            (255, 128, 128), (255, 128, 0), (128, 128, 0),
            (255, 255, 128), (128, 255, 255)
        ]
        
        # Emotion text mapping (é¿å…ç»ˆç«¯ä¸æ”¯æŒemoji)
        self.emotion_text = {
            'angry': 'Anger',
            'disgust': 'Disgust',
            'fear': 'Fear',
            'happy': 'Happiness',
            'sad': 'Sadness',
            'surprise': 'Surprise',
            'neutral': 'Neutral',
            # HSEmotion mappings
            'anger': 'Anger',
            'happiness': 'Happiness',
            'sadness': 'Sadness',
            'contempt': 'Contempt'
        }
        
        # Performance settings
        self.process_every_n_frames = 10  # ä»5æ”¹åˆ°10ï¼Œæå‡æ€§èƒ½
        self.emotion_every_n_frames = 10  # Emotion slower
        self.frame_counter = 0
        self.cached_results = {}
        
        # Age smoothing - ä¿å­˜æœ€è¿‘Næ¬¡å¹´é¾„æ£€æµ‹ç»“æœ
        self.age_history = {}  # person_id: [age1, age2, ...]
        self.age_history_size = 5  # ä¿ç•™æœ€è¿‘5æ¬¡ç»“æœ
        
        # Emotion smoothing - ä¿å­˜æœ€è¿‘Næ¬¡æƒ…ç»ªæ£€æµ‹ç»“æœ
        self.emotion_history = {}  # person_id: [emotion1, emotion2, ...]
        self.emotion_history_size = 5  # ä¿ç•™æœ€è¿‘5æ¬¡ç»“æœï¼Œå–ä¼—æ•°
        
        # Auto-print descriptions
        self.auto_print_descriptions = False
        
        # AIæè¿°ç”Ÿæˆå™¨
        if AI_DESC_AVAILABLE:
            # å¯ä»¥åœ¨è¿™é‡Œé…ç½®ä½¿ç”¨å“ªä¸ªæä¾›å•†ï¼š'openai', 'claude', æˆ– 'none'
            # å¦‚æœè®¾ç½®äº†API keyï¼Œä¼šè‡ªåŠ¨å¯ç”¨
            self.ai_generator = AIDescriptionGenerator(provider='openai')
        else:
            self.ai_generator = None
        
        # Visual effects settings
        self.enable_effects = False  # ç‰¹æ•ˆå¼€å…³
        self.effect_mode = 'silhouette'  # ç‰¹æ•ˆæ¨¡å¼: 'silhouette' æˆ– 'ascii'
        # self.enable_depth = True  # é»˜è®¤å¼€å¯æ·±åº¦æ•ˆæœï¼ˆå½“ç‰¹æ•ˆå¼€å¯æ—¶ï¼‰- å·²æ³¨é‡Š
        self.feather_radius = 15  # ç¾½åŒ–åŠå¾„
        self.trail_frames = 5  # æ®‹å½±å¸§æ•°
        self.trail_history = []  # æ®‹å½±å†å²å¸§
        # self.depth_cache = None  # ç¼“å­˜æ·±åº¦å›¾ä»¥æé«˜æ€§èƒ½ - å·²æ³¨é‡Š
        # self.depth_cache_counter = 0  # æ·±åº¦å›¾æ›´æ–°è®¡æ•°å™¨ - å·²æ³¨é‡Š
        
        # ASCIIè‰ºæœ¯æ•ˆæœè®¾ç½®
        self.ascii_grid_size = 8  # å­—ç¬¦å¤§å°å’Œå¯†åº¦ï¼ˆæ›´å°=æ›´å¯†é›†ï¼‰
        self.ascii_threshold = 20  # äº®åº¦é˜ˆå€¼ï¼ˆæ›´ä½=æ›´å¤šå­—ç¬¦ï¼ŒåŒ…æ‹¬æš—è‰²è¡£æœï¼‰
        self.ascii_chars = ['0', '1']  # ä½¿ç”¨çš„å­—ç¬¦
        
        # æ‰«æçº¿æ•ˆæœï¼ˆæ¯ä¸ªäººç‰©ç‹¬ç«‹çš„æ‰«æçº¿ï¼‰
        self.scan_line_positions = {}  # person_id: y_position
        self.scan_line_trails = {}  # person_id: [å†å²ä½ç½®åˆ—è¡¨] ç”¨äºæ®‹å½±æ•ˆæœ
        self.scan_line_speed = 10  # æ‰«æçº¿ç§»åŠ¨é€Ÿåº¦ï¼ˆåƒç´ /å¸§ï¼Œæ›´å¿«ï¼Œå¿«ä¸¤å€ï¼‰
        self.scan_line_thickness = 1  # æ‰«æçº¿ç²—ç»†ï¼ˆ1pxæè¾¹ï¼‰
        self.scan_line_color = (255, 255, 255)  # æ‰«æçº¿é¢œè‰²ï¼ˆç™½è‰²ï¼‰
        self.scan_line_trail_frames = 8  # æ®‹å½±ä¿ç•™å¸§æ•°
        self.scan_line_blur_radius = 15  # æ®‹å½±æ¨¡ç³ŠåŠå¾„
        
        print("=" * 60)
        print("Features:")
        print("  âœ“ Body: 17 Keypoints, Clothing, Color, Body Type")
        if self.face_enabled:
            print("  âœ“ Face: Age")
        if self.emotion_enabled:
            print("  âœ“ Emotion: 8 expressions (via HSEmotion)")
        if self.segmentation_enabled:
            print("  âœ“ Visual Effects: Precise Segmentation (YOLOv8-Seg)")
        else:
            print("  âœ“ Visual Effects: Keypoint-based Silhouette")
        # if self.depth_enabled:
        #     print("  âœ“ Depth Effects: MiDaS depth estimation")  # å·²æ³¨é‡Š
        print("  âœ“ Visualization: Keypoints + Skeleton")
        print("=" * 60)
        
        # Initialize Oscilloscope Overlay (Audio-Reactive Visualization)
        # å»¶è¿ŸéŸ³é¢‘åˆå§‹åŒ–ï¼Œé¿å…ä¸å…¶ä»–ç¨‹åºå†²çª
        if OSCILLOSCOPE_AVAILABLE:
            try:
                print("\nåˆå§‹åŒ–éŸ³é¢‘ååº”ç¤ºæ³¢å™¨...")
                self.oscilloscope = OscilloscopeOverlay(
                    width=OSCILLOSCOPE_WIDTH,
                    height=OSCILLOSCOPE_HEIGHT,
                    position=OSCILLOSCOPE_POSITION
                )
                
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨éŸ³é¢‘
                if ENABLE_OSCILLOSCOPE_AUDIO:
                    print("  âœ“ ç¤ºæ³¢å™¨å·²åˆ›å»º (éŸ³é¢‘å»¶è¿Ÿå¯åŠ¨)")
                    print("  ğŸ’¡ æç¤º: ç¤ºæ³¢å™¨éŸ³é¢‘å°†åœ¨æ£€æµ‹åˆ°äººåè‡ªåŠ¨å¯åŠ¨")
                else:
                    print("  âœ“ ç¤ºæ³¢å™¨å·²åˆ›å»º (éŸ³é¢‘å·²ç¦ç”¨)")
                    print("  ğŸ’¡ æç¤º: åœ¨ oscilloscope_config_user.py ä¸­å¯ç”¨éŸ³é¢‘")
                
                self.oscilloscope_audio_started = False
                self.oscilloscope_audio_enabled = ENABLE_OSCILLOSCOPE_AUDIO
            except Exception as e:
                print(f"  âœ— ç¤ºæ³¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.oscilloscope = None
                self.oscilloscope_audio_started = False
                self.oscilloscope_audio_enabled = False
        else:
            self.oscilloscope = None
            self.oscilloscope_audio_started = False
            self.oscilloscope_audio_enabled = False
        
        print("=" * 60)
    
    def detect_persons(self, frame):
        """Detect persons with pose"""
        results = self.yolo_model(frame, verbose=False, device=self.device)
        persons = []
        
        for result in results:
            if result.boxes is not None:
                for i, box in enumerate(result.boxes):
                    # ä¸¥æ ¼è¿‡æ»¤ï¼šåªæœ‰ç½®ä¿¡åº¦ > 0.75 æ‰è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„äºº
                    if int(box.cls[0]) == 0 and float(box.conf[0]) > 0.75:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        
                        keypoints = None
                        if result.keypoints is not None and len(result.keypoints.data) > i:
                            keypoints = result.keypoints.data[i].cpu().numpy()
                        
                        persons.append({
                            'bbox': (x1, y1, x2, y2),
                            'confidence': float(box.conf[0]),
                            'keypoints': keypoints
                        })
        
        return persons
    
    def analyze_faces(self, frame):
        """Analyze all faces using InsightFace"""
        if not self.face_enabled or not hasattr(self, 'face_app'):
            return []
        
        try:
            faces = self.face_app.get(frame)
            face_results = []
            
            for face in faces:
                bbox = face.bbox.astype(int)
                age = int(face.age)
                embedding = face.embedding
                landmarks = face.kps.astype(int)
                
                face_results.append({
                    'bbox': tuple(bbox),
                    'age': age,
                    'embedding': embedding,
                    'landmarks': landmarks
                })
            
            return face_results
        except Exception as e:
            print(f"InsightFace error: {e}")
            return []
    
    def detect_emotion(self, face_region):
        """Detect emotion using DeepFace or FER"""
        # print(f"DEBUG: detect_emotion called. DeepFace available: {DEEPFACE_AVAILABLE}")
        if not DEEPFACE_AVAILABLE and not self.emotion_enabled:
            return None, None
        
        try:
            # Try DeepFace first (if available)
            if DEEPFACE_AVAILABLE:
                # print("DEBUG: Calling DeepFace...")
                try:
                    result = DeepFace.analyze(
                        img_path=face_region,
                        actions=['emotion'],
                        enforce_detection=False,
                        detector_backend='skip',
                        silent=True
                    )
                    
                    if isinstance(result, list):
                        result = result[0]
                    
                    if 'dominant_emotion' in result:
                        dominant_emotion = result['dominant_emotion']
                        confidence = result['emotion'][dominant_emotion] / 100.0  # Convert to 0-1
                        # print(f"DEBUG: DeepFace success: {dominant_emotion}")
                        return dominant_emotion, confidence
                except Exception as e:
                    print(f"DeepFace runtime error: {e}")
                    pass
            
            # Fallback to FER if available
            if self.emotion_enabled and hasattr(self, 'emotion_detector'):
                # print(f"FER Debug: Input shape {face_region.shape}")
                emotions = self.emotion_detector.detect_emotions(face_region)
                # print(f"FER Debug: Result {emotions}")
                if emotions and len(emotions) > 0:
                    emotion_scores = emotions[0]['emotions']
                    dominant_emotion = max(emotion_scores, key=emotion_scores.get)
                    confidence = emotion_scores[dominant_emotion]
                    return dominant_emotion, confidence
                else:
                    # Try forcing it if detection fails? No direct API for that in simple FER.
                    pass
            
            return None, None
        except Exception as e:
            print(f"Emotion detection error: {e}")
            return None, None
    
    def match_face_to_person(self, person_bbox, face_bbox):
        """Check if face belongs to person (relaxed matching)"""
        px1, py1, px2, py2 = person_bbox
        fx1, fy1, fx2, fy2 = face_bbox
        
        # Calculate face center
        face_cx = (fx1 + fx2) / 2
        face_cy = (fy1 + fy2) / 2
        
        # æ”¾å®½åˆ¤å®šï¼šåªè¦äººè„¸ä¸­å¿ƒåœ¨èº«ä½“æ¡†çš„æ°´å¹³èŒƒå›´å†…
        if px1 <= face_cx <= px2:
            person_height = py2 - py1
            # è¿™é‡Œçš„ 0.6 æ”¹æˆ 0.9ï¼Œé˜²æ­¢å› ä¸ºæŠ¬å¤´æˆ–æ‹æ‘„è§’åº¦å¯¼è‡´äººè„¸ä½ç½®åé«˜è€Œè¢«è¿‡æ»¤
            if py1 - person_height * 0.2 <= face_cy <= py1 + person_height * 0.9:
                return True
                
        return False
    
    def smooth_age(self, person_id, raw_age):
        """Smooth age using moving average"""
        if person_id not in self.age_history:
            self.age_history[person_id] = []
        
        # Add new age to history
        self.age_history[person_id].append(raw_age)
        
        # Keep only recent N values
        if len(self.age_history[person_id]) > self.age_history_size:
            self.age_history[person_id] = self.age_history[person_id][-self.age_history_size:]
        
        # Return average
        return int(sum(self.age_history[person_id]) / len(self.age_history[person_id]))
    
    def smooth_emotion(self, person_id, raw_emotion):
        """Smooth emotion using voting (mode)"""
        if person_id not in self.emotion_history:
            self.emotion_history[person_id] = []
        
        # Add new emotion to history
        self.emotion_history[person_id].append(raw_emotion)
        
        # Keep only recent N values
        if len(self.emotion_history[person_id]) > self.emotion_history_size:
            self.emotion_history[person_id] = self.emotion_history[person_id][-self.emotion_history_size:]
        
        # Return most frequent emotion (mode)
        from collections import Counter
        counts = Counter(self.emotion_history[person_id])
        return counts.most_common(1)[0][0]
    
    def analyze_body_type(self, keypoints, bbox):
        """Analyze body type from keypoints"""
        if keypoints is None or len(keypoints) < 17:
            return None
        
        try:
            x1, y1, x2, y2 = bbox
            person_height = y2 - y1
            person_width = x2 - x1
            
            # Shoulder width
            left_shoulder = keypoints[5]
            right_shoulder = keypoints[6]
            if left_shoulder[2] > 0.3 and right_shoulder[2] > 0.3:
                shoulder_width = abs(right_shoulder[0] - left_shoulder[0])
            else:
                shoulder_width = person_width * 0.6
            
            # Hip width
            left_hip = keypoints[11]
            right_hip = keypoints[12]
            if left_hip[2] > 0.3 and right_hip[2] > 0.3:
                hip_width = abs(right_hip[0] - left_hip[0])
            else:
                hip_width = person_width * 0.5
            
            # Calculate ratios
            height_width_ratio = person_height / person_width if person_width > 0 else 2.0
            shoulder_hip_ratio = shoulder_width / hip_width if hip_width > 0 else 1.0
            
            # Body type classification
            if height_width_ratio > 2.2 and shoulder_hip_ratio > 1.1:
                build = "Athletic"
            elif height_width_ratio > 2.0:
                build = "Slim"
            elif height_width_ratio < 1.8:
                build = "Broad"
            else:
                build = "Average"
            
            if shoulder_hip_ratio > 1.15:
                shape_type = "V-Shape"
            elif shoulder_hip_ratio < 0.95:
                shape_type = "A-Shape"
            else:
                shape_type = "Rectangle"
            
            return {
                'build': build,
                'shape': shape_type
            }
        except:
            return None
    
    def get_color(self, image_region, mask=None):
        """Extract dominant color using HSV + Mask filtering (More Robust)"""
        if image_region is None or image_region.size == 0:
            return None, 0.0
        
        try:
            # 1. Pre-processing: Resize for speed
            target_size = (64, 64)
            img_small = cv2.resize(image_region, target_size)
            
            # 2. Masking: Only use pixels inside the person silhouette
            if mask is not None:
                mask_small = cv2.resize(mask, target_size)
                # Binarize mask
                _, mask_bin = cv2.threshold(mask_small, 128, 255, cv2.THRESH_BINARY)
                # Extract valid pixels (BGR)
                pixels = img_small[mask_bin > 0]
                
                # If too few pixels (e.g. empty mask), fallback to whole image or return None
                if len(pixels) < 50:
                    # Fallback: use center crop if mask failed
                    h, w = img_small.shape[:2]
                    center_pixels = img_small[h//4:h*3//4, w//4:w*3//4].reshape(-1, 3)
                    pixels = center_pixels
            else:
                # No mask provided, use whole image
                pixels = img_small.reshape(-1, 3)
            
            if len(pixels) == 0:
                return None, 0.0

            # 3. K-Means clustering to find Dominant Color (in BGR space)
            # n_init='auto' is default in newer sklearn, using fixed number for compatibility
            kmeans = KMeans(n_clusters=1, random_state=42, n_init=3)
            kmeans.fit(pixels)
            dominant_bgr = kmeans.cluster_centers_[0].astype(int)
            
            # Calculate confidence based on compactness (inertia)
            # Normalized by number of pixels
            inertia = kmeans.inertia_
            avg_dist = inertia / len(pixels) if len(pixels) > 0 else 0
            # Heuristic: avg_dist < 500 is very pure, > 3000 is mixed
            color_confidence = max(0.0, min(1.0, 1.0 - (avg_dist / 4000.0)))
            
            # 4. Convert Dominant Color to HSV for robust classification
            # Create a 1x1 pixel to convert color space
            pixel_bgr = np.uint8([[dominant_bgr]])
            pixel_hsv = cv2.cvtColor(pixel_bgr, cv2.COLOR_BGR2HSV)[0][0]
            
            h_val, s_val, v_val = int(pixel_hsv[0]), int(pixel_hsv[1]), int(pixel_hsv[2])
            
            # === HSV Color Classification Rules ===
            # OpenCV HSV ranges: H: 0-179, S: 0-255, V: 0-255
            
            # 1. Achromatic Colors (Black, White, Gray)
            # Check saturation and value
            
            # Black: Very low value (dark)
            if v_val < 40: 
                return 'Black', color_confidence
            
            # White: Very low saturation AND high value (bright)
            if s_val < 30 and v_val > 200:
                return 'White', color_confidence
                
            # Gray: Low saturation, medium value
            if s_val < 40:
                return 'Gray', color_confidence
            
            # 2. Chromatic Colors (based on Hue)
            # H values are halved degrees (0-360 -> 0-179)
            
            if (0 <= h_val <= 10) or (160 <= h_val <= 179):
                return 'Red', color_confidence
            elif 11 <= h_val <= 25:
                return 'Orange', color_confidence
            elif 26 <= h_val <= 35:
                return 'Yellow', color_confidence
            elif 36 <= h_val <= 85:
                return 'Green', color_confidence
            elif 86 <= h_val <= 99:
                return 'Cyan', color_confidence
            elif 100 <= h_val <= 130:
                return 'Blue', color_confidence
            elif 131 <= h_val <= 150:
                return 'Purple', color_confidence
            elif 151 <= h_val <= 159:
                return 'Pink', color_confidence
            
            return 'Mixed', color_confidence
            
        except Exception as e:
            # print(f"Color extraction error: {e}")
            return None, 0.0
    
    def classify_clothing_type(self, person_roi, keypoints, upper_roi, lower_roi):
        """Classify clothing type based on visual features"""
        try:
            if person_roi is None or person_roi.size == 0:
                return {'upper': 'Top', 'lower': 'Bottom'}
            
            h, w = person_roi.shape[:2]
            
            # Analyze upper and lower regions
            upper_type = "Top"
            lower_type = "Bottom"
            
            # Check if it's a dress (uniform color from top to bottom)
            if upper_roi is not None and lower_roi is not None:
                upper_gray = cv2.cvtColor(upper_roi, cv2.COLOR_BGR2GRAY) if len(upper_roi.shape) == 3 else upper_roi
                lower_gray = cv2.cvtColor(lower_roi, cv2.COLOR_BGR2GRAY) if len(lower_roi.shape) == 3 else lower_roi
                
                # Calculate color similarity
                upper_mean = np.mean(upper_gray)
                lower_mean = np.mean(lower_gray)
                color_diff = abs(upper_mean - lower_mean)
                
                # If very similar, might be a dress
                if color_diff < 25:
                    return {'upper': 'Dress', 'lower': None}
            
            # Analyze upper clothing
            if keypoints is not None and len(keypoints) >= 17:
                # Check sleeve length based on elbow/wrist visibility
                left_elbow = keypoints[7]
                right_elbow = keypoints[8]
                left_wrist = keypoints[9]
                right_wrist = keypoints[10]
                
                # If wrists are visible, likely short sleeves
                if (left_wrist[2] > 0.5 or right_wrist[2] > 0.5):
                    upper_type = "T-shirt"
                # If elbows visible but not wrists, likely long sleeves
                elif (left_elbow[2] > 0.5 or right_elbow[2] > 0.5):
                    upper_type = "Shirt"
                else:
                    upper_type = "Top"
            
            # Analyze lower clothing
            if keypoints is not None and len(keypoints) >= 17:
                left_knee = keypoints[13]
                right_knee = keypoints[14]
                left_ankle = keypoints[15]
                right_ankle = keypoints[16]
                
                # Check if knees and ankles are visible
                knees_visible = (left_knee[2] > 0.5 or right_knee[2] > 0.5)
                ankles_visible = (left_ankle[2] > 0.5 or right_ankle[2] > 0.5)
                
                if knees_visible and ankles_visible:
                    # Both visible, likely pants
                    lower_type = "Pants"
                elif knees_visible:
                    # Only knees visible, might be shorts
                    lower_type = "Shorts"
                else:
                    lower_type = "Bottom"
            
            return {'upper': upper_type, 'lower': lower_type}
            
        except Exception as e:
            return {'upper': 'Top', 'lower': 'Bottom'}
    
    def generate_person_description(self, result):
        """Generate a natural language description of the person"""
        # å¦‚æœå¯ç”¨äº†AIç”Ÿæˆå™¨ï¼Œä½¿ç”¨AIç”Ÿæˆæ›´ä¸°å¯Œçš„æè¿°
        if self.ai_generator and self.ai_generator.enabled:
            try:
                # å‡†å¤‡æ•°æ®
                person_data = {
                    'age': result.get('face', {}).get('smoothed_age', result.get('face', {}).get('age')),
                    'emotion': result.get('emotion'),
                    'body_type': result.get('body_type'),
                    'clothing': result.get('clothing'),
                    'person_id': result.get('person_id')
                }
                
                # ä½¿ç”¨AIç”Ÿæˆ
                ai_desc = self.ai_generator.generate_description(person_data)
                if ai_desc:
                    return ai_desc
            except Exception as e:
                print(f"AIæè¿°ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•ç‰ˆæœ¬: {e}")
        
        # Fallback to original simple version (English)
        parts = []
        
        # Age
        if result.get('face'):
            face = result['face']
            age = face.get('smoothed_age', face['age'])
            
            # Age group
            if age < 18:
                age_group = "young"
            elif age < 35:
                age_group = "young adult"
            elif age < 55:
                age_group = "middle-aged"
            else:
                age_group = "senior"
            
            parts.append(f"A {age_group} person in their {age//10*10}s")
        else:
            parts.append("A person")
        
        # Body type
        if result.get('body_type'):
            build = result['body_type']['build'].lower()
            if build != 'average':
                parts.append(f"with a {build} build")
        
        # Clothing
        clothing_parts = []
        if result.get('clothing'):
            clothing = result['clothing']
            clothing_type = clothing.get('type')
            
            if clothing_type:
                upper_type = clothing_type.get('upper', '').lower()
                lower_type = clothing_type.get('lower', '').lower() if clothing_type.get('lower') else None
                
                # å®‰å…¨è·å–é¢œè‰²å¹¶è½¬å°å†™
                upper_color_raw = clothing.get('upper_color')
                lower_color_raw = clothing.get('lower_color')
                
                upper_color = upper_color_raw.lower() if upper_color_raw else None
                lower_color = lower_color_raw.lower() if lower_color_raw else None
                
                # Upper clothing
                if upper_type == 'dress':
                    if upper_color:
                        clothing_parts.append(f"wearing a {upper_color} dress")
                    else:
                        clothing_parts.append("wearing a dress")
                else:
                    upper_desc = ""
                    if upper_color:
                        upper_desc = f"a {upper_color} {upper_type if upper_type else 'top'}"
                    elif upper_type:
                        upper_desc = f"a {upper_type}"
                    
                    # Lower clothing
                    lower_desc = ""
                    if lower_type:
                        if lower_color:
                            lower_desc = f"{lower_color} {lower_type}"
                        else:
                            lower_desc = lower_type
                    
                    if upper_desc and lower_desc:
                        clothing_parts.append(f"wearing {upper_desc} and {lower_desc}")
                    elif upper_desc:
                        clothing_parts.append(f"wearing {upper_desc}")
                    elif lower_desc:
                        clothing_parts.append(f"wearing {lower_desc}")
        
        if clothing_parts:
            parts.append(clothing_parts[0])
        
        # Emotion
        if result.get('emotion'):
            emotion = result['emotion'].lower()
            emotion_text_map = {
                'happy': 'smiling',
                'sad': 'looking sad',
                'angry': 'looking angry',
                'surprise': 'looking surprised',
                'fear': 'looking fearful',
                'disgust': 'looking disgusted',
                'neutral': 'with a neutral expression'
            }
            emotion_desc = emotion_text_map.get(emotion, f'feeling {emotion}')
            parts.append(emotion_desc)
        
        # Join all parts
        if len(parts) == 1:
            return parts[0] + "."
        elif len(parts) == 2:
            return f"{parts[0]}, {parts[1]}."
        else:
            # Join with commas and 'and' before last part if it's emotion
            base = ", ".join(parts[:-1])
            return f"{base}, {parts[-1]}."
    
    def draw_keypoints_and_skeleton(self, frame, keypoints):
        """Draw keypoints and skeleton"""
        if keypoints is None or len(keypoints) < 17:
            return
        
        # Draw skeleton
        if self.show_skeleton:
            for connection in self.skeleton:
                pt1_idx, pt2_idx = connection
                
                if pt1_idx < len(keypoints) and pt2_idx < len(keypoints):
                    pt1 = keypoints[pt1_idx]
                    pt2 = keypoints[pt2_idx]
                    
                    if pt1[2] > 0.3 and pt2[2] > 0.3:
                        x1, y1 = int(pt1[0]), int(pt1[1])
                        x2, y2 = int(pt2[0]), int(pt2[1])
                        cv2.line(frame, (x1, y1), (x2, y2), (255, 255, 255), 1)
        
        # Draw keypoints
        if self.show_keypoints:
            for idx, kpt in enumerate(keypoints):
                x, y, conf = kpt
                
                if conf > 0.3:
                    x, y = int(x), int(y)
                    # 4x4 white circle (radius 2)
                    cv2.circle(frame, (x, y), 2, (255, 255, 255), -1)
    
    def get_text_color_for_position(self, person_mask, x, y):
        """
        æ ¹æ®ä½ç½®åˆ¤æ–­æ–‡æœ¬é¢œè‰²
        åœ¨é»‘è‰²å‰ªå½±ä¸Šè¿”å›ç™½è‰²ï¼Œåœ¨ç™½è‰²èƒŒæ™¯ä¸Šè¿”å›é»‘è‰²
        """
        h, w = person_mask.shape[:2]
        # ç¡®ä¿åæ ‡åœ¨èŒƒå›´å†…
        x = max(0, min(int(x), w-1))
        y = max(0, min(int(y), h-1))
        
        # æ£€æŸ¥è¯¥ä½ç½®çš„maskå€¼
        mask_value = person_mask[y, x]
        
        # å¦‚æœmaskå€¼ > 128ï¼ˆåœ¨é»‘è‰²å‰ªå½±åŒºåŸŸï¼‰ï¼Œè¿”å›ç™½è‰²
        # å¦åˆ™è¿”å›é»‘è‰²ï¼ˆåœ¨ç™½è‰²èƒŒæ™¯ä¸Šï¼‰
        if mask_value > 128:
            return (255, 255, 255)  # ç™½è‰²æ–‡å­—ï¼ˆåœ¨é»‘è‰²å‰ªå½±ä¸Šï¼‰
        else:
            return (0, 0, 0)  # é»‘è‰²æ–‡å­—ï¼ˆåœ¨ç™½è‰²èƒŒæ™¯ä¸Šï¼‰
    
    def draw_keypoints_skeleton_red(self, frame, keypoints):
        """
        ç»˜åˆ¶çº¢è‰²çš„å…³èŠ‚ç‚¹å’Œéª¨æ¶ï¼ˆç‰¹æ•ˆæ¨¡å¼ä¸“ç”¨ï¼Œéšè—é¢éƒ¨å…³èŠ‚ç‚¹ï¼‰
        """
        # é¢éƒ¨å…³é”®ç‚¹ç´¢å¼•ï¼š0=Nose, 1=LeftEye, 2=RightEye, 3=LeftEar, 4=RightEar
        face_keypoint_indices = {0, 1, 2, 3, 4}
        
        # Draw skeletonï¼ˆè·³è¿‡æ¶‰åŠé¢éƒ¨çš„è¿çº¿ï¼‰
        if self.show_skeleton:
            for connection in self.skeleton:
                pt1_idx, pt2_idx = connection
                
                # è·³è¿‡é¢éƒ¨è¿çº¿
                if pt1_idx in face_keypoint_indices or pt2_idx in face_keypoint_indices:
                    continue
                
                if pt1_idx < len(keypoints) and pt2_idx < len(keypoints):
                    pt1 = keypoints[pt1_idx]
                    pt2 = keypoints[pt2_idx]
                    
                    if pt1[2] > 0.3 and pt2[2] > 0.3:
                        x1, y1 = int(pt1[0]), int(pt1[1])
                        x2, y2 = int(pt2[0]), int(pt2[1])
                        cv2.line(frame, (x1, y1), (x2, y2), (0, 0, 255), 1)  # çº¢è‰²
        
        # Draw keypointsï¼ˆè·³è¿‡é¢éƒ¨å…³èŠ‚ç‚¹ï¼‰
        if self.show_keypoints:
            for idx, kpt in enumerate(keypoints):
                # è·³è¿‡é¢éƒ¨å…³é”®ç‚¹
                if idx in face_keypoint_indices:
                    continue
                
                x, y, conf = kpt
                
                if conf > 0.3:
                    x, y = int(x), int(y)
                    cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)  # çº¢è‰²
    
    def draw_keypoints_skeleton_adaptive(self, frame, person_mask, keypoints):
        """
        åœ¨ç‰¹æ•ˆå¸§ä¸Šç»˜åˆ¶å…³èŠ‚ç‚¹å’Œéª¨æ¶ï¼Œé¢œè‰²æ ¹æ®èƒŒæ™¯è‡ªé€‚åº”
        """
        if keypoints is None or len(keypoints) < 17:
            return
        
        # Draw skeleton
        if self.show_skeleton:
            for connection in self.skeleton:
                pt1_idx, pt2_idx = connection
                
                if pt1_idx < len(keypoints) and pt2_idx < len(keypoints):
                    pt1 = keypoints[pt1_idx]
                    pt2 = keypoints[pt2_idx]
                    
                    if pt1[2] > 0.3 and pt2[2] > 0.3:
                        x1, y1 = int(pt1[0]), int(pt1[1])
                        x2, y2 = int(pt2[0]), int(pt2[1])
                        
                        # åœ¨ä¸­ç‚¹ä½ç½®é‡‡æ ·é¢œè‰²
                        mid_x = (x1 + x2) // 2
                        mid_y = (y1 + y2) // 2
                        line_color = self.get_text_color_for_position(person_mask, mid_x, mid_y)
                        
                        cv2.line(frame, (x1, y1), (x2, y2), line_color, 1)
        
        # Draw keypoints
        if self.show_keypoints:
            for idx, kpt in enumerate(keypoints):
                x, y, conf = kpt
                
                if conf > 0.3:
                    x, y = int(x), int(y)
                    # æ ¹æ®å…³èŠ‚ç‚¹ä½ç½®é€‰æ‹©é¢œè‰²
                    point_color = self.get_text_color_for_position(person_mask, x, y)
                    # 4x4 circle (radius 2)
                    cv2.circle(frame, (x, y), 2, point_color, -1)
    
    def draw_data_blocks(self, effect_frame, person_mask, results, original_frame, target_result=None):
        """
        åœ¨äººç‰©å‰ªå½±ä¸Šç»˜åˆ¶å¤§é‡æ•°æ®æ–¹å—
        Target: ç™½è‰²è½®å»“æ ¼å­ + é»‘è‰²èƒŒæ™¯æ ¼å­
        Others: é»‘è‰²è½®å»“æ ¼å­ + é»‘è‰²èƒŒæ™¯æ ¼å­
        """
        h, w = effect_frame.shape[:2]
        
        for r in results:
            if 'bbox' not in r:
                continue
            
            # Check if this person is the active target
            is_target = (target_result is not None and r is target_result)
            
            x1, y1, x2, y2 = r['bbox']
            
            # === æ‰©å¤§ Box èŒƒå›´ä»¥åŒ…å«ä¼¸å±•çš„æ‰‹éƒ¨ ===
            # å·¦å³å„æ‰©å¤§ 15%ï¼Œä¸Šä¸‹å„æ‰©å¤§ 10%
            w_box = x2 - x1
            h_box = y2 - y1
            pad_w = int(w_box * 0.15)
            pad_h = int(h_box * 0.1)
            
            x1 = max(0, int(x1 - pad_w))
            y1 = max(0, int(y1 - pad_h))
            x2 = min(w, int(x2 + pad_w))
            y2 = min(h, int(y2 + pad_h))
            
            person_width = x2 - x1
            person_height = y2 - y1
            
            # åˆ›å»ºå¯†é›†çš„ç½‘æ ¼æ–¹å—ï¼Œå‡ ä¹é“ºæ»¡äººç‰©åŒºåŸŸ
            # æ–¹å—å¤§å°å’Œé—´è·å›ºå®šï¼Œç¡®ä¿æ‰€æœ‰æ ¼å­é—´è·ä¸€è‡´
            base_size = max(4, min(person_width, person_height) // 20)  # åŸºç¡€å¤§å°ï¼ˆæ›´å°ï¼‰
            spacing = base_size + 2  # æ–¹å—é—´è·ï¼ˆå›ºå®šï¼‰
            size = base_size  # å›ºå®šå¤§å°ï¼ˆä¸æ ¹æ®ä½ç½®å˜åŒ–ï¼‰
            
            # è®¡ç®—ç½‘æ ¼æ•°é‡ï¼ˆå‡ ä¹é“ºæ»¡ï¼‰
            grid_cols = max(3, person_width // spacing)
            grid_rows = max(3, person_height // spacing)
            
            # åœ¨äººç‰©bboxåŒºåŸŸå†…åˆ›å»ºç½‘æ ¼
            for row in range(grid_rows):
                for col in range(grid_cols):
                    # è®¡ç®—æ–¹å—ä½ç½®ï¼ˆåœ¨bboxå†…ï¼Œä½¿ç”¨å›ºå®šé—´è·ï¼‰
                    block_x = x1 + col * spacing + spacing // 2
                    block_y = y1 + row * spacing + spacing // 2
                    
                    # ç¡®ä¿åœ¨bboxå†…
                    if block_x < x1 or block_x >= x2 or block_y < y1 or block_y >= y2:
                        continue
                    
                    # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                    block_x = max(size, min(block_x, w - size))
                    block_y = max(size, min(block_y, h - size))
                    
                    # è®¡ç®—æ–¹å—åŒºåŸŸ
                    block_x1 = block_x - size // 2
                    block_y1 = block_y - size // 2
                    block_x2 = block_x + size // 2
                    block_y2 = block_y + size // 2
                    
                    # æ£€æŸ¥æ–¹å—ä¸­å¿ƒä½ç½®æ˜¯åœ¨å‰ªå½±å†…è¿˜æ˜¯èƒŒæ™¯ä¸Š
                    # é‡‡æ ·æ–¹å—ä¸­å¿ƒåŠå››ä¸ªè§’çš„maskå€¼
                    sample_points = [
                        (block_x, block_y),  # ä¸­å¿ƒ
                        (block_x1, block_y1),  # å·¦ä¸Š
                        (block_x2, block_y1),  # å³ä¸Š
                        (block_x1, block_y2),  # å·¦ä¸‹
                        (block_x2, block_y2),  # å³ä¸‹
                    ]
                    
                    mask_values = []
                    for px, py in sample_points:
                        px = max(0, min(int(px), w-1))
                        py = max(0, min(int(py), h-1))
                        mask_values.append(person_mask[py, px])
                    
                    # å¦‚æœå¤šæ•°ç‚¹åœ¨å‰ªå½±å†…ï¼ˆmask > 128ï¼‰ï¼Œä½¿ç”¨ç™½è‰²ï¼ˆå¦‚æœæ˜¯ç›®æ ‡ï¼‰ï¼›å¦åˆ™ä½¿ç”¨é»‘è‰²
                    in_silhouette = sum(1 for v in mask_values if v > 128) >= len(mask_values) // 2
                    
                    if in_silhouette:
                        block_color = (255, 255, 255) if is_target else (0, 0, 0)
                    else:
                        block_color = (0, 0, 0)
                    
                    # ç»˜åˆ¶æ–¹å—ï¼ˆå¡«å……ï¼Œæ— æè¾¹ï¼‰
                    cv2.rectangle(effect_frame, (block_x1, block_y1), (block_x2, block_y2), 
                                 block_color, -1)
    
    def draw_scan_line(self, effect_frame, results):
        """
        åœ¨æ¯ä¸ªäººç‰©çš„bboxå†…ç»˜åˆ¶ä»ä¸Šåˆ°ä¸‹çš„æ‰«æçº¿æ•ˆæœï¼ˆå¸¦æ¨¡ç³Šæ‹–å½±ï¼‰
        ä½¿ç”¨æ··åˆæ¨¡å¼å•ç‹¬å åŠ ï¼Œä¸å½±å“ä¸‹é¢çš„å†…å®¹
        """
        h, w = effect_frame.shape[:2]
        
        # åˆ›å»ºå•ç‹¬çš„æ‰«æçº¿å›¾å±‚ï¼ˆå…¨é€æ˜ï¼‰
        scan_layer = np.zeros((h, w, 3), dtype=np.uint8)
        
        for r in results:
            if 'bbox' not in r:
                continue
            
            x1, y1, x2, y2 = r['bbox']
            person_id = r.get('person_id', 0)
            
            # ç¡®ä¿bboxåæ ‡åœ¨å›¾åƒèŒƒå›´å†…
            x1 = max(0, int(x1))
            y1 = max(0, int(y1))
            x2 = min(w, int(x2))
            y2 = min(h, int(y2))
            
            # ä¸ºæ¯ä¸ªäººç‰©ç»´æŠ¤ç‹¬ç«‹çš„æ‰«æçº¿ä½ç½®å’Œæ®‹å½±å†å²
            if person_id not in self.scan_line_positions:
                self.scan_line_positions[person_id] = y1  # ä»bboxé¡¶éƒ¨å¼€å§‹
                self.scan_line_trails[person_id] = []  # åˆå§‹åŒ–æ®‹å½±åˆ—è¡¨
            
            # æ›´æ–°æ‰«æçº¿ä½ç½®ï¼ˆåœ¨bboxå†…ï¼‰
            self.scan_line_positions[person_id] += self.scan_line_speed
            
            # å¦‚æœæ‰«æçº¿åˆ°è¾¾bboxåº•éƒ¨ï¼Œé‡ç½®åˆ°bboxé¡¶éƒ¨
            if self.scan_line_positions[person_id] >= y2:
                self.scan_line_positions[person_id] = y1
                # é‡ç½®æ—¶æ¸…ç©ºæ®‹å½±
                self.scan_line_trails[person_id] = []
            
            # æ·»åŠ å½“å‰ä½ç½®åˆ°æ®‹å½±å†å²
            scan_y = int(self.scan_line_positions[person_id])
            if y1 <= scan_y < y2:  # ç¡®ä¿åœ¨bboxå†…
                self.scan_line_trails[person_id].append({
                    'y': scan_y,
                    'x1': x1,
                    'x2': x2
                })
                
                # é™åˆ¶æ®‹å½±å†å²é•¿åº¦
                if len(self.scan_line_trails[person_id]) > self.scan_line_trail_frames:
                    self.scan_line_trails[person_id].pop(0)
            
            # åˆ›å»ºä¸´æ—¶å›¾åƒç”¨äºç»˜åˆ¶æ®‹å½±ï¼ˆåªåœ¨bboxåŒºåŸŸå†…ï¼‰
            bbox_width = x2 - x1
            bbox_height = y2 - y1
            if bbox_width > 0 and bbox_height > 0:
                # åˆ›å»ºä¸´æ—¶å›¾åƒï¼ˆåªåŒ…å«bboxåŒºåŸŸï¼‰
                trail_mask = np.zeros((bbox_height, bbox_width, 3), dtype=np.uint8)
                
                # åœ¨ä¸´æ—¶å›¾åƒä¸Šç»˜åˆ¶æ‰€æœ‰æ®‹å½±çº¿ï¼ˆç™½è‰²ï¼Œé€æ¸å˜æ·¡ï¼‰
                trail_list = self.scan_line_trails[person_id]
                for i, trail in enumerate(trail_list):
                    # è®¡ç®—é€æ˜åº¦ï¼šè¶Šæ–°çš„æ®‹å½±è¶Šäº®
                    alpha = (i + 1) / len(trail_list) if trail_list else 1.0
                    # è®¡ç®—é¢œè‰²å¼ºåº¦ï¼ˆç™½è‰²ï¼‰
                    color_intensity = int(255 * alpha)
                    trail_color = (color_intensity, color_intensity, color_intensity)  # BGRæ ¼å¼ï¼Œç™½è‰²
                    
                    # è®¡ç®—åœ¨ä¸´æ—¶å›¾åƒä¸­çš„ç›¸å¯¹ä½ç½®
                    trail_y_rel = trail['y'] - y1
                    trail_x1_rel = trail['x1'] - x1
                    trail_x2_rel = trail['x2'] - x1
                    
                    # ç¡®ä¿åœ¨bboxèŒƒå›´å†…
                    if 0 <= trail_y_rel < bbox_height:
                        trail_x1_rel = max(0, min(trail_x1_rel, bbox_width - 1))
                        trail_x2_rel = max(0, min(trail_x2_rel, bbox_width - 1))
                        cv2.line(trail_mask, (trail_x1_rel, trail_y_rel), 
                                (trail_x2_rel, trail_y_rel), trail_color, self.scan_line_thickness)
                
                # å¯¹æ®‹å½±åº”ç”¨é«˜æ–¯æ¨¡ç³Šï¼ˆåˆ›å»ºæ‹–å½±æ•ˆæœï¼‰
                if len(trail_list) > 0:
                    blurred_trail = cv2.GaussianBlur(trail_mask, 
                                                     (self.scan_line_blur_radius * 2 + 1, 
                                                      self.scan_line_blur_radius * 2 + 1), 0)
                    
                    # å°†æ¨¡ç³Šåçš„æ®‹å½±ç»˜åˆ¶åˆ°æ‰«æçº¿å›¾å±‚ï¼ˆä¸æ··åˆï¼Œç›´æ¥ç»˜åˆ¶ï¼‰
                    scan_layer[y1:y2, x1:x2] = np.maximum(scan_layer[y1:y2, x1:x2], blurred_trail)
            
            # ç»˜åˆ¶å½“å‰æ‰«æçº¿åˆ°æ‰«æçº¿å›¾å±‚ï¼ˆç™½è‰²ï¼Œä¸æ¨¡ç³Šï¼Œæ¸…æ™°ï¼‰
            scan_y = int(self.scan_line_positions[person_id])
            if y1 <= scan_y < y2:  # ç¡®ä¿åœ¨bboxå†…
                cv2.line(scan_layer, (x1, scan_y), (x2, scan_y), 
                        self.scan_line_color, self.scan_line_thickness)
        
        # ä½¿ç”¨ Screen æ··åˆæ¨¡å¼å åŠ æ‰«æçº¿å›¾å±‚ï¼ˆåªå¢äº®ï¼Œä¸å½±å“é»‘è‰²ï¼‰
        # Screen æ¨¡å¼ï¼šresult = 1 - (1 - base) * (1 - overlay)
        # å¯¹äºç™½è‰²æ‰«æçº¿ï¼Œä¼šå¢äº®ä¸‹é¢çš„å†…å®¹ï¼Œä½†é»‘è‰²ä¿æŒä¸å˜
        scan_layer_float = scan_layer.astype(np.float32) / 255.0
        effect_frame_float = effect_frame.astype(np.float32) / 255.0
        
        # Screen æ··åˆæ¨¡å¼
        blended = 1.0 - (1.0 - effect_frame_float) * (1.0 - scan_layer_float)
        blended = (blended * 255.0).astype(np.uint8)
        
        # å°†æ··åˆåçš„ç»“æœå†™å›æ•ˆæœå¸§
        effect_frame[:] = blended
    
    def draw_info_on_effect_frame(self, effect_frame, person_mask, results):
        """
        åœ¨ç‰¹æ•ˆå¸§ä¸Šç»˜åˆ¶è¯†åˆ«ä¿¡æ¯ï¼ˆè¾¹ç•Œæ¡†ã€äººè„¸æ¡†ã€å…³èŠ‚ç‚¹ã€éª¨æ¶ã€æ–‡æœ¬ï¼‰ï¼Œæ ¹æ®èƒŒæ™¯è‡ªåŠ¨è°ƒæ•´é¢œè‰²
        """
        font = cv2.FONT_HERSHEY_SIMPLEX
        h, w = effect_frame.shape[:2]
        
        for idx, r in enumerate(results):
            if 'bbox' not in r:
                continue
                
            x1, y1, x2, y2 = r['bbox']
            
            # === åŒæ­¥ draw_data_blocks çš„æ‰©å¤§é€»è¾‘ï¼Œç¡®ä¿æ¡†èƒ½åŒ…ä½æ ¼å­ ===
            w_box = x2 - x1
            h_box = y2 - y1
            pad_w = int(w_box * 0.15)
            pad_h = int(h_box * 0.1)
            
            x1 = max(0, int(x1 - pad_w))
            y1 = max(0, int(y1 - pad_h))
            x2 = min(w, int(x2 + pad_w))
            y2 = min(h, int(y2 + pad_h))
            # ========================================================
            
            # 1. ç»˜åˆ¶äººä½“è¾¹ç•Œæ¡†ï¼ˆè‡ªé€‚åº”é¢œè‰²ï¼‰
            # é‡‡æ ·è¾¹ç•Œæ¡†å››ä¸ªè§’çš„é¢œè‰²ï¼Œå–å¤šæ•°
            corners = [
                (x1, y1), (x2, y1), (x1, y2), (x2, y2)
            ]
            colors = [self.get_text_color_for_position(person_mask, x, y) for x, y in corners]
            # ç»Ÿè®¡ç™½è‰²å’Œé»‘è‰²çš„æ•°é‡
            white_count = sum(1 for c in colors if c == (255, 255, 255))
            bbox_color = (255, 255, 255) if white_count >= 2 else (0, 0, 0)
            cv2.rectangle(effect_frame, (x1, y1), (x2, y2), bbox_color, 1)
            
            # 2. ç»˜åˆ¶æ–‡æœ¬ä¿¡æ¯ï¼ˆä»ä¸Šå¾€ä¸‹æ’åˆ—ï¼šPerson -> Age -> Emotion -> Build -> Clothesï¼‰
            y_offset = y1 - 10
            
            # Clothingï¼ˆæœ€åç»˜åˆ¶ï¼Œæ˜¾ç¤ºåœ¨æœ€ä¸‹æ–¹ï¼‰
            if r.get('clothing'):
                clothing = r['clothing']
                clothing_type = clothing.get('type', {})
                
                if clothing_type:
                    upper_type = clothing_type.get('upper', 'Top')
                    lower_type = clothing_type.get('lower', 'Bottom')
                    
                    # Lower clothingï¼ˆæ ¼å¼ï¼šClothes: Pants, Blackï¼‰- å…ˆç»˜åˆ¶ï¼Œæ˜¾ç¤ºåœ¨ä¸‹
                    if lower_type:
                        lower_text = f"Clothes: {lower_type}"
                        if clothing.get('lower_color'):
                            lower_text += f", {clothing['lower_color']}"
                        text_color = (0, 0, 0)  # æ”¹å›é»‘è‰²
                        cv2.putText(effect_frame, lower_text, (x1, y_offset),
                                   font, 1.2, text_color, 2)
                        y_offset -= 35
                    
                    # Upper clothingï¼ˆæ ¼å¼ï¼šClothes: T-Shirt, Grayï¼‰- åç»˜åˆ¶ï¼Œæ˜¾ç¤ºåœ¨ä¸Š
                    upper_text = f"Clothes: {upper_type}"
                    if clothing.get('upper_color'):
                        upper_text += f", {clothing['upper_color']}"
                    text_color = (0, 0, 0)  # æ”¹å›é»‘è‰²
                    cv2.putText(effect_frame, upper_text, (x1, y_offset),
                               font, 1.2, text_color, 2)
                    y_offset -= 35
                else:
                    # Fallback to simple color displayï¼ˆæ ¼å¼ï¼šClothes: Top, Colorï¼‰
                    if clothing.get('lower_color'):
                        lower_text = f"Clothes: Bottom, {clothing['lower_color']}"
                        text_color = (0, 0, 0)  # æ”¹å›é»‘è‰²
                        cv2.putText(effect_frame, lower_text, (x1, y_offset),
                                   font, 1.2, text_color, 2)
                        y_offset -= 35
                    
                    if clothing.get('upper_color'):
                        upper_text = f"Clothes: Top, {clothing['upper_color']}"
                        text_color = (0, 0, 0)  # æ”¹å›é»‘è‰²
                        cv2.putText(effect_frame, upper_text, (x1, y_offset),
                                   font, 1.2, text_color, 2)
                        y_offset -= 35
            
            # Body typeï¼ˆç‰¹æ•ˆæ¨¡å¼ä¸‹ä½¿ç”¨é»‘è‰²ï¼‰
            if r.get('body_type'):
                body_type = r['body_type']
                build_text = f"Build: {body_type.get('build', 'Unknown')}"
                # æ·»åŠ ç½®ä¿¡åº¦ï¼ˆåŸºäºå…³é”®ç‚¹å¯è§åº¦ï¼‰
                if r.get('keypoints') is not None:
                    keypoints = r['keypoints']
                    visible_kpts = sum(1 for kpt in keypoints if kpt[2] > 0.3)
                    body_conf = visible_kpts / 17.0
                    build_text += f" ({body_conf*100:.0f}%)"
                text_color = (0, 0, 0)  # æ”¹å›é»‘è‰²
                cv2.putText(effect_frame, build_text, (x1, y_offset),
                           font, 1.2, text_color, 2)
                y_offset -= 35
            
            # Emotionï¼ˆç‰¹æ•ˆæ¨¡å¼ä¸‹ä½¿ç”¨é»‘è‰²ï¼‰
            emotion = r.get('emotion')
            emotion_display = self.emotion_text.get(emotion, 'Neutral') if emotion else 'Analyzing...'
            emotion_text = f"Emotion: {emotion_display}"
            if emotion and r.get('emotion_conf'):
                emotion_text += f" ({r['emotion_conf']*100:.0f}%)"
            text_color = (0, 0, 0)  # æ”¹å›é»‘è‰²
            cv2.putText(effect_frame, emotion_text, (x1, y_offset),
                       font, 1.2, text_color, 2)
            y_offset -= 35
            
            # Ageï¼ˆç‰¹æ•ˆæ¨¡å¼ä¸‹ä½¿ç”¨é»‘è‰²ï¼‰
            if r.get('face'):
                face = r['face']
                display_age = face.get('smoothed_age', face.get('age', 0))
                age_text = f"Age: {display_age}y"
                text_color = (0, 0, 0)  # æ”¹å›é»‘è‰²
                cv2.putText(effect_frame, age_text, (x1, y_offset),
                           font, 1.2, text_color, 2)
                y_offset -= 35
            
            # Person IDï¼ˆæœ€åç»˜åˆ¶ï¼Œæ˜¾ç¤ºåœ¨æœ€ä¸Šæ–¹ï¼‰
            person_conf = r.get('person_conf', 0.0)
            person_text = f"Person {r.get('person_id', idx+1)} ({person_conf*100:.0f}%)"
            text_color = (0, 0, 0)  # æ”¹å›é»‘è‰²
            cv2.putText(effect_frame, person_text, (x1, y_offset),
                       font, 1.2, text_color, 2)
            
            # 3. ç»˜åˆ¶äººè„¸è¾¹ç•Œæ¡†ï¼ˆçº¢è‰²ï¼Œæœ€åç»˜åˆ¶ï¼Œæ˜¾ç¤ºåœ¨æœ€ä¸Šå±‚ï¼‰
            # å·²æ ¹æ®ç”¨æˆ·è¦æ±‚ç§»é™¤
            # if r.get('face'):
            #     face = r['face']
            #     if 'bbox' in face:
            #         fx1, fy1, fx2, fy2 = face['bbox']
            #         cv2.rectangle(effect_frame, (fx1, fy1), (fx2, fy2), (0, 0, 255), 1)  # çº¢è‰²
            
            # 4. ç»˜åˆ¶å…³èŠ‚ç‚¹å’Œéª¨æ¶ï¼ˆçº¢è‰²ï¼Œæœ€åç»˜åˆ¶ï¼Œæ˜¾ç¤ºåœ¨æœ€ä¸Šå±‚ï¼‰
            # å·²æ ¹æ®ç”¨æˆ·è¦æ±‚ç§»é™¤
            # if 'keypoints' in r and r['keypoints'] is not None:
            #     self.draw_keypoints_skeleton_red(effect_frame, r['keypoints'])
    
    def create_person_silhouette_from_keypoints(self, keypoints, bbox, img_h, img_w):
        """
        ä»å…³é”®ç‚¹åˆ›å»ºç²¾ç¡®çš„äººä½“è½®å»“
        ä½¿ç”¨å…³é”®ç‚¹è¿æ¥å’Œå½¢æ€å­¦æ“ä½œæ¥åˆ›å»ºæ›´å‡†ç¡®çš„äººå½¢
        """
        mask = np.zeros((img_h, img_w), dtype=np.uint8)
        
        if keypoints is None or len(keypoints) < 17:
            # å¦‚æœæ²¡æœ‰å…³é”®ç‚¹ï¼Œä½¿ç”¨bboxåˆ›å»ºæ¤­åœ†
            if bbox:
                x1, y1, x2, y2 = bbox
                center_x = int((x1 + x2) / 2)
                center_y = int((y1 + y2) / 2)
                width = int(x2 - x1)
                height = int(y2 - y1)
                cv2.ellipse(mask, (center_x, center_y), (width//2, height//2), 0, 0, 360, 255, -1)
            return mask
        
        # æå–å¯è§å…³é”®ç‚¹åŠå…¶ç´¢å¼•
        visible_kpts = {}
        for idx, kpt in enumerate(keypoints):
            if len(kpt) >= 3 and kpt[2] > 0.3:  # ç½®ä¿¡åº¦ > 0.3
                x, y = int(kpt[0]), int(kpt[1])
                x = max(0, min(x, img_w-1))
                y = max(0, min(y, img_h-1))
                visible_kpts[idx] = (x, y)
        
        if len(visible_kpts) < 5:
            # å…³é”®ç‚¹å¤ªå°‘ï¼Œä½¿ç”¨bbox
            if bbox:
                x1, y1, x2, y2 = bbox
                center_x = int((x1 + x2) / 2)
                center_y = int((y1 + y2) / 2)
                width = int(x2 - x1)
                height = int(y2 - y1)
                cv2.ellipse(mask, (center_x, center_y), (width//2, height//2), 0, 0, 360, 255, -1)
            return mask
        
        # æ–¹æ³•1: ä½¿ç”¨å…³é”®ç‚¹è¿æ¥åˆ›å»ºè½®å»“
        # å®šä¹‰èº«ä½“éƒ¨ä½çš„å…³é”®ç‚¹ç»„ï¼ˆç”¨äºåˆ›å»ºæ›´ç²¾ç¡®çš„è½®å»“ï¼‰
        body_parts = {
            'head': [0, 1, 2, 3, 4],  # å¤´éƒ¨
            'torso': [5, 6, 11, 12],  # èº¯å¹²
            'left_arm': [5, 7, 9],  # å·¦è‡‚
            'right_arm': [6, 8, 10],  # å³è‡‚
            'left_leg': [11, 13, 15],  # å·¦è…¿
            'right_leg': [12, 14, 16],  # å³è…¿
        }
        
        # ä¸ºæ¯ä¸ªèº«ä½“éƒ¨ä½åˆ›å»ºè½®å»“
        for part_name, part_indices in body_parts.items():
            part_points = []
            for idx in part_indices:
                if idx in visible_kpts:
                    part_points.append(visible_kpts[idx])
            
            if len(part_points) >= 2:
                part_points = np.array(part_points, dtype=np.int32)
                
                # å¯¹äºå¤´éƒ¨ï¼Œä½¿ç”¨æ¤­åœ†
                if part_name == 'head' and len(part_points) >= 3:
                    # è®¡ç®—å¤´éƒ¨è¾¹ç•Œ
                    min_x, min_y = part_points.min(axis=0)
                    max_x, max_y = part_points.max(axis=0)
                    center_x = (min_x + max_x) // 2
                    center_y = (min_y + max_y) // 2
                    width = max(20, max_x - min_x)
                    height = max(20, max_y - min_y)
                    cv2.ellipse(mask, (center_x, center_y), (width//2, height//2), 0, 0, 360, 255, -1)
                # å¯¹äºèº¯å¹²ï¼Œä½¿ç”¨å‡¸åŒ…
                elif part_name == 'torso' and len(part_points) >= 3:
                    hull = cv2.convexHull(part_points)
                    cv2.fillPoly(mask, [hull], 255)
                # å¯¹äºå››è‚¢ï¼Œä½¿ç”¨è¿æ¥çº¿åŠ å®½åº¦
                elif len(part_points) >= 2:
                    # ç»˜åˆ¶è¿æ¥çº¿ï¼Œå¹¶åŠ ç²—
                    for i in range(len(part_points) - 1):
                        pt1 = tuple(part_points[i])
                        pt2 = tuple(part_points[i + 1])
                        # æ ¹æ®èº«ä½“éƒ¨ä½è®¾ç½®ä¸åŒçš„çº¿å®½
                        if part_name in ['left_arm', 'right_arm']:
                            thickness = 25
                        elif part_name in ['left_leg', 'right_leg']:
                            thickness = 30
                        else:
                            thickness = 20
                        cv2.line(mask, pt1, pt2, 255, thickness)
                    # åœ¨å…³é”®ç‚¹ä½ç½®ç»˜åˆ¶åœ†
                    for pt in part_points:
                        cv2.circle(mask, tuple(pt), thickness//2, 255, -1)
        
        # æ–¹æ³•2: ä½¿ç”¨æ‰€æœ‰å…³é”®ç‚¹çš„å‡¸åŒ…ä½œä¸ºåŸºç¡€ï¼Œç„¶åç»†åŒ–
        all_points = np.array(list(visible_kpts.values()), dtype=np.int32)
        if len(all_points) >= 3:
            hull = cv2.convexHull(all_points)
            # åˆ›å»ºä¸´æ—¶maskç”¨äºå‡¸åŒ…
            temp_mask = np.zeros((img_h, img_w), dtype=np.uint8)
            cv2.fillPoly(temp_mask, [hull], 255)
            
            # å°†å‡¸åŒ…ä¸èº«ä½“éƒ¨ä½maskåˆå¹¶
            mask = cv2.bitwise_or(mask, temp_mask)
        
        # æ–¹æ³•3: ä½¿ç”¨å½¢æ€å­¦æ“ä½œå¹³æ»‘å’Œå¡«å……è½®å»“
        # å…ˆè†¨èƒ€å†è…èš€ï¼Œå¡«å……å°æ´
        kernel = np.ones((5, 5), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)
        # è½»å¾®è…èš€ä»¥å¹³æ»‘è¾¹ç¼˜
        mask = cv2.erode(mask, kernel, iterations=1)
        mask = cv2.dilate(mask, kernel, iterations=1)
        
        return mask
    
    # æ·±åº¦ä¼°è®¡å‡½æ•°å·²æ³¨é‡Š
    # def estimate_depth(self, frame):
    #     """
    #     ä½¿ç”¨MiDaSä¼°è®¡åœºæ™¯æ·±åº¦
    #     è¿”å›å½’ä¸€åŒ–çš„æ·±åº¦å›¾ (0-255)
    #     """
    #     if not self.depth_enabled:
    #         return None
    #     
    #     try:
    #         # æ¯3å¸§æ›´æ–°ä¸€æ¬¡æ·±åº¦å›¾ä»¥æé«˜æ€§èƒ½
    #         self.depth_cache_counter += 1
    #         if self.depth_cache is not None and self.depth_cache_counter % 3 != 0:
    #             return self.depth_cache
    #         
    #         # å‡†å¤‡è¾“å…¥
    #         img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    #         input_batch = self.midas_transform(img_rgb).to(self.device)
    #         
    #         # æ¨ç†
    #         with torch.no_grad():
    #             prediction = self.midas(input_batch)
    #             
    #             # è°ƒæ•´åˆ°åŸå§‹åˆ†è¾¨ç‡
    #             prediction = F.interpolate(
    #                 prediction.unsqueeze(1),
    #                 size=frame.shape[:2],
    #                 mode="bicubic",
    #                 align_corners=False,
    #             ).squeeze()
    #         
    #         # è½¬æ¢ä¸ºnumpyå¹¶å½’ä¸€åŒ–åˆ°0-255
    #         depth_map = prediction.cpu().numpy()
    #         
    #         # å½’ä¸€åŒ– (è¿‘å¤„é«˜å€¼ï¼Œè¿œå¤„ä½å€¼)
    #         depth_min = depth_map.min()
    #         depth_max = depth_map.max()
    #         if depth_max > depth_min:
    #             depth_normalized = (depth_map - depth_min) / (depth_max - depth_min)
    #             depth_normalized = (depth_normalized * 255).astype(np.uint8)
    #         else:
    #             depth_normalized = np.zeros_like(depth_map, dtype=np.uint8)
    #         
    #         # åè½¬æ·±åº¦å€¼ï¼ˆè®©è¿‘çš„åœ°æ–¹æ›´äº®ï¼‰
    #         depth_normalized = 255 - depth_normalized
    #         
    #         # ç¼“å­˜æ·±åº¦å›¾
    #         self.depth_cache = depth_normalized
    #         
    #         return depth_normalized
    #         
    #     except Exception as e:
    #         print(f"Depth estimation error: {e}")
    #         return None
    
    def get_segmentation_mask(self, frame, conf_threshold=0.75):
        """
        ä½¿ç”¨YOLOv8-Segè·å–ç²¾ç¡®çš„äººä½“åˆ†å‰²mask
        Args:
            frame: è¾“å…¥å›¾åƒ
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œåªæœ‰é«˜äºæ­¤å€¼çš„æ‰ä¼šè¢«åˆ†å‰² (é»˜è®¤ 0.75)
        """
        if not self.segmentation_enabled:
            return None
        
        try:
            # ä½¿ç”¨åˆ†å‰²æ¨¡å‹ï¼Œç›´æ¥ä¼ å…¥ç½®ä¿¡åº¦é˜ˆå€¼è¿›è¡Œè¿‡æ»¤
            seg_results = self.yolo_seg_model(frame, verbose=False, conf=conf_threshold)
            
            if seg_results and len(seg_results) > 0:
                seg_result = seg_results[0]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†å‰²mask
                if seg_result.masks is not None and len(seg_result.masks.data) > 0:
                    h, w = frame.shape[:2]
                    combined_mask = np.zeros((h, w), dtype=np.uint8)
                    
                    # éå†æ‰€æœ‰æ£€æµ‹åˆ°çš„å¯¹è±¡
                    for i, (box, mask_data) in enumerate(zip(seg_result.boxes, seg_result.masks.data)):
                        # åªå¤„ç†personç±»åˆ« (class_id = 0 in COCO)
                        class_id = int(box.cls.cpu().numpy()[0])
                        if class_id == 0:  # person
                            # è·å–maskå¹¶è°ƒæ•´åˆ°åŸå§‹å›¾åƒå¤§å°
                            mask = mask_data.cpu().numpy()
                            mask_resized = cv2.resize(mask, (w, h), interpolation=cv2.INTER_LINEAR)
                            
                            # è½¬æ¢ä¸ºäºŒå€¼mask (0-255)
                            mask_binary = (mask_resized > 0.5).astype(np.uint8) * 255
                            
                            # åˆå¹¶åˆ°æ€»mask
                            combined_mask = cv2.bitwise_or(combined_mask, mask_binary)
                    
                    return combined_mask
        except Exception as e:
            print(f"Segmentation error: {e}")
        
        return None
    
    def create_ascii_effect(self, frame, person_mask, results):
        """
        åˆ›å»ºASCIIè‰ºæœ¯æ•ˆæœï¼šä½¿ç”¨0å’Œ1å­—ç¬¦æ˜¾ç¤ºäººç‰©è½®å»“
        èƒŒæ™¯ç”¨å¤æ‚ç¬¦å·å¡«æ»¡ï¼ˆç°è‰²ï¼‰
        """
        h, w = frame.shape[:2]
        
        # åˆ›å»ºé»‘è‰²èƒŒæ™¯
        ascii_frame = np.zeros((h, w, 3), dtype=np.uint8)
        
        # éå†ç½‘æ ¼ï¼Œç»˜åˆ¶å­—ç¬¦
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = self.ascii_grid_size / 30.0  # æ ¹æ®ç½‘æ ¼å¤§å°è°ƒæ•´å­—ä½“
        font_thickness = 1
        
        # èƒŒæ™¯å­—ç¬¦é›†ï¼ˆå¤æ‚ç¬¦å·ï¼‰
        background_chars = ['.', ':', '-', '=', '+', '*', '#', '%', '@', '~', '^', '&']
        # ç°è‰² #666666 = RGB(102, 102, 102)
        background_color = (102, 102, 102)  # BGRæ ¼å¼
        
        for y in range(0, h, self.ascii_grid_size):
            for x in range(0, w, self.ascii_grid_size):
                # æ£€æŸ¥å½“å‰ä½ç½®æ˜¯å¦åœ¨äººç‰©maskå†…
                if x < w and y < h:
                    mask_value = person_mask[y, x]
                    
                    # å¦‚æœåœ¨äººç‰©åŒºåŸŸå†…ï¼ˆmask > 128ï¼‰
                    if mask_value > 128:
                        # è·å–è¯¥ä½ç½®çš„äº®åº¦
                        pixel = frame[y, x]
                        brightness = (int(pixel[0]) + int(pixel[1]) + int(pixel[2])) / 3
                        
                        # åªæœ‰å½“äº®åº¦è¶…è¿‡é˜ˆå€¼æ—¶æ‰ç»˜åˆ¶å­—ç¬¦
                        if brightness > self.ascii_threshold:
                            # éšæœºé€‰æ‹©0æˆ–1
                            char = random.choice(self.ascii_chars)
                            
                            # ç»˜åˆ¶ç™½è‰²å­—ç¬¦ï¼ˆäººç‰©ï¼‰
                            cv2.putText(ascii_frame, char, (x, y + self.ascii_grid_size),
                                       font, font_scale, (255, 255, 255), font_thickness)
                    else:
                        # èƒŒæ™¯åŒºåŸŸï¼šç»˜åˆ¶ç°è‰²å¤æ‚ç¬¦å·
                        char = random.choice(background_chars)
                        cv2.putText(ascii_frame, char, (x, y + self.ascii_grid_size),
                                   font, font_scale, background_color, font_thickness)
        
        # ä¸ç»˜åˆ¶ä»»ä½•æ ‡æ³¨ä¿¡æ¯ï¼Œä¿æŒçº¯ç²¹çš„ASCIIè‰ºæœ¯æ•ˆæœ
        
        return ascii_frame
    
    def draw_info_on_ascii_frame(self, ascii_frame, results):
        """
        åœ¨ASCIIå¸§ä¸Šç»˜åˆ¶è¯†åˆ«ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼Œåªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯ï¼‰
        """
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        for idx, r in enumerate(results):
            if 'bbox' not in r:
                continue
                
            x1, y1, x2, y2 = r['bbox']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†ï¼ˆç»¿è‰²ï¼‰
            cv2.rectangle(ascii_frame, (x1, y1), (x2, y2), (0, 255, 0), 1)
            
            # ç»˜åˆ¶å…³é”®ä¿¡æ¯
            y_offset = y1 - 10
            
            # Age
            if r.get('face'):
                face = r['face']
                display_age = face.get('smoothed_age', face.get('age', 0))
                cv2.putText(ascii_frame, f"Age: {display_age}y", (x1, y_offset),
                           font, 0.4, (0, 255, 0), 1)
                y_offset -= 12
            
            # Person ID
            person_conf = r.get('person_conf', 0.0)
            cv2.putText(ascii_frame, f"Person {r.get('person_id', idx+1)} ({person_conf*100:.0f}%)", 
                       (x1, y_offset), font, 0.4, (0, 255, 0), 1)
    
    def apply_visual_effects(self, frame, results, person_mask=None, target_person_idx=None):
        """
        åº”ç”¨è§†è§‰ç‰¹æ•ˆï¼šé»‘è‰²å‰ªå½± + çœŸå®èƒŒæ™¯ + æ•°æ®æ–¹å— æˆ– ASCIIè‰ºæœ¯
        """
        if not self.enable_effects:
            return frame
        
        # 1. è¿‡æ»¤ç»“æœ (åªä¿ç•™é«˜ç½®ä¿¡åº¦)
        filtered_results = [r for r in results if r.get('person_conf', 0) > 0.75]
        
        # 2. ç¡®å®šè¿½è¸ªç›®æ ‡å¯¹è±¡
        target_result = None
        if target_person_idx is not None and 0 <= target_person_idx < len(results):
            target_result = results[target_person_idx]
        
        h, w = frame.shape[:2]
        
        # ä½¿ç”¨çœŸå®æ‘„åƒå¤´èƒŒæ™¯
        effect_frame = frame.copy()
        
        # 3. è·å– Mask (æ‰€æœ‰äºº)
        if person_mask is None:
            person_mask = self.get_segmentation_mask(frame, conf_threshold=0.75)
        
        if person_mask is None:
            person_mask = np.zeros((h, w), dtype=np.uint8)
            for r in filtered_results:
                bbox = r.get('bbox')
                keypoints = r.get('keypoints')
                person_silhouette = self.create_person_silhouette_from_keypoints(
                    keypoints, bbox, h, w
                )
                person_mask = cv2.bitwise_or(person_mask, person_silhouette)
        
        # 4. æ¸²æŸ“
        if self.effect_mode == 'ascii':
            effect_frame = self.create_ascii_effect(frame, person_mask, filtered_results)
            return effect_frame
        else:
            # é»˜è®¤å‰ªå½±æ¨¡å¼
            original_mask = person_mask.copy()
            
            if self.feather_radius > 0:
                person_mask = cv2.GaussianBlur(person_mask, 
                                             (self.feather_radius * 2 + 1, self.feather_radius * 2 + 1),
                                             0)
            
            # ç»˜åˆ¶æ•°æ®æ–¹å— (ä¼ å…¥ target_result ä»¥åŒºåˆ†é¢œè‰²)
            self.draw_data_blocks(effect_frame, original_mask, filtered_results, frame, target_result=target_result)
            
            # ç»˜åˆ¶è¯†åˆ«ä¿¡æ¯
            self.draw_info_on_effect_frame(effect_frame, original_mask, filtered_results)
            
            return effect_frame
        
        # æ ¹æ®ç‰¹æ•ˆæ¨¡å¼é€‰æ‹©ä¸åŒçš„æ¸²æŸ“æ–¹æ³•
        if self.effect_mode == 'ascii':
            # ASCIIè‰ºæœ¯æ¨¡å¼ (ä½¿ç”¨è¿‡æ»¤åçš„ results)
            effect_frame = self.create_ascii_effect(frame, person_mask, filtered_results)
            return effect_frame
        else:
            # é»˜è®¤å‰ªå½±æ¨¡å¼
            # ä¿å­˜åŸå§‹maskç”¨äºæ–‡æœ¬é¢œè‰²åˆ¤æ–­ï¼ˆåœ¨ç¾½åŒ–ä¹‹å‰ï¼‰
            original_mask = person_mask.copy()
            
            # åº”ç”¨ç¾½åŒ–æ•ˆæœï¼ˆè¾¹ç¼˜æ¨¡ç³Šï¼‰
            if self.feather_radius > 0:
                person_mask = cv2.GaussianBlur(person_mask, 
                                             (self.feather_radius * 2 + 1, self.feather_radius * 2 + 1),
                                             0)
            
            # ä¸ç»˜åˆ¶çº¯é»‘è‰²å‰ªå½±ï¼Œåªç»˜åˆ¶é»‘è‰²æ ¼å­
            # effect_frame ä¿æŒçœŸå®èƒŒæ™¯ï¼Œä¸åšä»»ä½•æ··åˆ
            
            # æ·»åŠ æ®‹å½±æ•ˆæœï¼ˆåœ¨ç»˜åˆ¶å®Œæ‰€æœ‰å†…å®¹ååº”ç”¨ï¼Œé¿å…å½±å“æ ¼å­é¢œè‰²ï¼‰
            # æ³¨æ„ï¼šæ®‹å½±æ•ˆæœä¼šåœ¨æœ€ååº”ç”¨ï¼Œæ‰€ä»¥ä¸ä¼šå½±å“æ ¼å­çš„ç™½è‰²
            
            # åœ¨äººç‰©å‰ªå½±ä¸Šå åŠ æ•°æ®æ–¹å—ï¼ˆé©¬èµ›å…‹æ ¼å­æ•ˆæœï¼‰
            # æ³¨æ„ï¼šä¼ å…¥ filtered_results ç¡®ä¿åªç»˜åˆ¶é«˜ç½®ä¿¡åº¦äººç‰©çš„æ ¼å­
            self.draw_data_blocks(effect_frame, original_mask, filtered_results, frame)
            
            # åœ¨ç‰¹æ•ˆå¸§ä¸Šç»˜åˆ¶è¯†åˆ«ä¿¡æ¯ï¼ˆä½¿ç”¨é»‘è‰²æ–‡æœ¬ï¼Œåœ¨æ ¼å­ä¹‹åç»˜åˆ¶ï¼Œç¡®ä¿å¯è§ï¼‰
            self.draw_info_on_effect_frame(effect_frame, original_mask, filtered_results)
            
            # ç»˜åˆ¶æ‰«æçº¿æ•ˆæœï¼ˆåœ¨æ¯ä¸ªäººç‰©çš„bboxå†…ï¼‰
            # self.draw_scan_line(effect_frame, filtered_results)  # å·²æ³¨é‡Š
            
            return effect_frame
    
    def process_frame(self, frame):
        """Complete analysis of the frame"""
        self.frame_counter += 1
        
        # Detect persons
        persons = self.detect_persons(frame)
        
        # Analyze faces
        faces = self.analyze_faces(frame)
        
        should_analyze_body = (self.frame_counter % self.process_every_n_frames == 0)
        should_analyze_emotion = (self.frame_counter % self.emotion_every_n_frames == 0)
        
        results = []
        
        for idx, person in enumerate(persons):
            x1, y1, x2, y2 = person['bbox']
            keypoints = person['keypoints']
            person_roi = frame[y1:y2, x1:x2].copy()
            
            person_id = f"{x1//50}_{y1//50}"
            
            # Find matching face
            matching_face = None
            for face in faces:
                if self.match_face_to_person(person['bbox'], face['bbox']):
                    matching_face = face
                    # Smooth age to reduce jumping
                    raw_age = face['age']
                    smoothed_age = self.smooth_age(person_id, raw_age)
                    face['smoothed_age'] = smoothed_age
                    break
            
            # ç¡®ä¿person_idåœ¨ç¼“å­˜ä¸­ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼Œç«‹å³åˆå§‹åŒ–å¹¶åˆ†æä¸€æ¬¡ï¼Œé¿å…é—ªçƒï¼‰
            is_new_person = person_id not in self.cached_results
            if is_new_person:
                self.cached_results[person_id] = {
                    'body_type': None,
                    'upper_color': None,
                    'lower_color': None,
                    'clothing_type': None,
                    'emotion': None,
                    'emotion_conf': None,
                    'frame': self.frame_counter
                }
                # æ–°æ£€æµ‹åˆ°çš„äººç‰©ï¼Œç«‹å³åˆ†æä¸€æ¬¡ï¼Œé¿å…é—ªçƒ
                should_analyze_body_local = True
                should_analyze_emotion_local = True
            else:
                should_analyze_body_local = should_analyze_body
                should_analyze_emotion_local = should_analyze_emotion
            
            # Person æ£€æµ‹ç½®ä¿¡åº¦ï¼ˆæ¥è‡ªYOLOï¼‰
            person_conf = person.get('confidence', 0.0)
            
            # ä»ç¼“å­˜è¯»å–
            cached = self.cached_results[person_id]
            body_type = cached.get('body_type')
            upper_color = cached.get('upper_color')
            upper_color_conf = cached.get('upper_color_conf', 0.0)
            lower_color = cached.get('lower_color')
            lower_color_conf = cached.get('lower_color_conf', 0.0)
            clothing_type = cached.get('clothing_type')
            emotion = cached.get('emotion')
            emotion_conf = cached.get('emotion_conf')
            
            # Analyze face attributes (independent of body analysis)
            if matching_face:
                fx1, fy1, fx2, fy2 = matching_face['bbox']
                
                # Add padding for better emotion detection
                h, w = frame.shape[:2]
                pad_x = int((fx2 - fx1) * 0.2)
                pad_y = int((fy2 - fy1) * 0.2)
                
                fx1_pad = max(0, fx1 - pad_x)
                fy1_pad = max(0, fy1 - pad_y)
                fx2_pad = min(w, fx2 + pad_x)
                fy2_pad = min(h, fy2 + pad_y)
                
                # [å…³é”®ä¿®æ”¹ 1] å¿…é¡»ä½¿ç”¨ .copy()ï¼Œå¦åˆ™ TensorFlow å¯èƒ½ä¼šæŠ¥é”™
                face_region = frame[fy1_pad:fy2_pad, fx1_pad:fx2_pad].copy()
                
                # Emotion detection - Using HSEmotion (EfficientNet)
                if HSEMOTION_AVAILABLE and self.emotion_enabled and face_region.size > 0 and self.frame_counter % 5 == 0:
                    try:
                        # predict_emotions returns (emotion_label, scores_list)
                        # emotion_label is like 'Happiness', 'Neutral', etc.
                        emotion, scores = self.emotion_detector.predict_emotions(face_region, logits=False)
                        
                        # Find max score for confidence
                        confidence = max(scores)
                        
                        # Normalize to lowercase for consistency
                        emotion_lower = emotion.lower()
                        
                        # Apply smoothing
                        smoothed_emotion = self.smooth_emotion(person_id, emotion_lower)
                        
                        # Update cache
                        self.cached_results[person_id]['emotion'] = smoothed_emotion
                        self.cached_results[person_id]['emotion_conf'] = confidence
                            
                    except Exception as e:
                        print(f"!!! HSEmotion ERROR (Person {idx+1}): {e}")
                        pass
            
            # Analyze body attributes
            if should_analyze_body_local:
                body_type = self.analyze_body_type(keypoints, person['bbox'])
                
                # Generate silhouette mask for accurate color extraction (Remove background)
                # This ensures we only analyze pixels belonging to the person
                full_mask = self.create_person_silhouette_from_keypoints(
                    keypoints, person['bbox'], frame.shape[0], frame.shape[1]
                )
                
                # Crop mask to person ROI
                person_mask_roi = full_mask[y1:y2, x1:x2]
                
                h = person_roi.shape[0]
                mid = h // 2
                upper_roi = person_roi[:mid, :]
                lower_roi = person_roi[mid:, :]
                
                # Split mask for upper/lower
                upper_mask = None
                lower_mask = None
                
                if person_mask_roi.shape[:2] == person_roi.shape[:2]:
                    upper_mask = person_mask_roi[:mid, :]
                    lower_mask = person_mask_roi[mid:, :]
                
                upper_color, upper_color_conf = self.get_color(upper_roi, upper_mask)
                lower_color, lower_color_conf = self.get_color(lower_roi, lower_mask)
                
                # Upper color filtering (Confidence threshold)
                if upper_color and upper_color_conf < 0.6:
                    upper_color = None
                
                # Lower color filtering (Confidence threshold)
                if lower_color and lower_color_conf < 0.6:
                    lower_color = None
                
                # Classify clothing type
                clothing_type = self.classify_clothing_type(person_roi, keypoints, upper_roi, lower_roi)
                
                # Update body cache
                self.cached_results[person_id]['body_type'] = body_type
                self.cached_results[person_id]['upper_color'] = upper_color
                self.cached_results[person_id]['upper_color_conf'] = upper_color_conf
                self.cached_results[person_id]['lower_color'] = lower_color
                self.cached_results[person_id]['lower_color_conf'] = lower_color_conf
                self.cached_results[person_id]['clothing_type'] = clothing_type
                self.cached_results[person_id]['frame'] = self.frame_counter
            
            # ========== DRAW VISUALIZATION ==========
            # ç»˜åˆ¶ä»£ç å·²ç§»é™¤ï¼Œç»Ÿä¸€åœ¨ apply_visual_effects ä¸­ç»˜åˆ¶
            
            result_data = {
                'person_id': idx + 1,
                'person_conf': person_conf,  # æ·»åŠ Personç½®ä¿¡åº¦
                'bbox': person['bbox'],
                'keypoints': keypoints,
                'body_type': body_type,
                'face': matching_face,
                'emotion': emotion,
                'emotion_conf': emotion_conf,
                'clothing': {
                    'type': clothing_type,
                    'upper_color': upper_color,
                    'upper_color_conf': upper_color_conf,
                    'lower_color': lower_color,
                    'lower_color_conf': lower_color_conf
                }
            }
            
            # Generate natural language description
            description = self.generate_person_description(result_data)
            result_data['description'] = description
            
            results.append(result_data)
        
        # Clean cache
        if self.frame_counter % 30 == 0:
            old_keys = [k for k, v in self.cached_results.items() 
                       if self.frame_counter - v.get('frame', 0) > 30]
            for k in old_keys:
                del self.cached_results[k]
                # Also clean age history
                if k in self.age_history:
                    del self.age_history[k]
        
        # åº”ç”¨è§†è§‰ç‰¹æ•ˆ
        if self.enable_effects:
            frame = self.apply_visual_effects(frame, results)
        
        # æ¸²æŸ“ç¤ºæ³¢å™¨ï¼ˆå³ä¸‹è§’ï¼‰
        if hasattr(self, 'oscilloscope') and self.oscilloscope is not None:
            # å»¶è¿Ÿå¯åŠ¨éŸ³é¢‘ï¼šåªåœ¨ç”¨æˆ·å¯ç”¨ä¸”ç¬¬ä¸€æ¬¡æ£€æµ‹åˆ°äººåæ‰å¯åŠ¨
            if (self.oscilloscope_audio_enabled and 
                not self.oscilloscope_audio_started and 
                len(results) > 0):
                print("\næ£€æµ‹åˆ°äººç‰©ï¼Œå¯åŠ¨ç¤ºæ³¢å™¨éŸ³é¢‘...")
                if self.oscilloscope.enable_audio():
                    print("  âœ“ ç¤ºæ³¢å™¨éŸ³é¢‘å·²å¯åŠ¨")
                    self.oscilloscope_audio_started = True
                else:
                    print("  âš  éŸ³é¢‘å¯åŠ¨å¤±è´¥ï¼Œç¤ºæ³¢å™¨å°†æ— éŸ³é¢‘ååº”")
                    # ä¸è®¾ä¸º Noneï¼Œä»ç„¶æ˜¾ç¤ºè§†è§‰æ•ˆæœ
                    self.oscilloscope_audio_started = True  # æ ‡è®°ä¸ºå·²å°è¯•ï¼Œé¿å…é‡å¤
            
            # å¦‚æœæ£€æµ‹åˆ°äººï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªäººçš„å…³é”®ç‚¹æ•°æ®
            if len(results) > 0 and 'keypoints' in results[0]:
                self.oscilloscope.update_person_data(
                    keypoints=results[0]['keypoints']
                )
            
            # æ¸²æŸ“ç¤ºæ³¢å™¨åˆ°å¸§ä¸Šï¼ˆä¼šè‡ªåŠ¨æ”¾åœ¨å³ä¸‹è§’ï¼‰
            frame = self.oscilloscope.render(frame)
        
        return frame, results

# Note: This module is used by the multi-camera system (main_3cameras_single.py)
# The single-camera main() function has been removed as we only use the 3-camera version

